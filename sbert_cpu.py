# =================================================================================
# SCRIPT: Szybkie Etykietowanie i Korekta Danych za pomocƒÖ Wyszukiwania Semantycznego
# Wersja zoptymalizowana pod kƒÖtem szybko≈õci na CPU (cel: < 3 godziny)
# =================================================================================

# Step 0: Instalacja niezbƒôdnych pakiet√≥w.
# Uruchom tƒô kom√≥rkƒô tylko raz.
# !pip install -q pandas==2.2.2 torch==2.3.1 sentence-transformers==3.0.1 scikit-learn==1.5.0

import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
import sys
from collections import defaultdict

print("--- Start: Szybki proces etykietowania i korekty RELEVANT (zoptymalizowany dla CPU) ---")

# --- KONFIGURACJA (WYMAGANA AKCJA) ---
# 1. Ustaw ≈õcie≈ºkƒô do swojego pliku z danymi.
FILE_PATH = "your_dataset.csv"  # <--- ZMIE≈É NA ≈öCIE≈ªKƒò DO TWOJEGO PLIKU

# 2. Potwierd≈∫ nazwy kolumn.
TEXT_COLUMN = "TEXT"
LABEL_COLUMN = "DEMAND_ID"
RELEVANT_COLUMN = "RELEVANT"

# 3. Ustaw pr√≥g pewno≈õci (podobie≈Ñstwa semantycznego) do przypisania etykiety.
# Dla tej metody pr√≥g mo≈ºe byƒá ni≈ºszy. Dobry punkt startowy to 0.5.
# Je≈õli dostaniesz za ma≈Ço etykiet, zmniejsz go. Je≈õli za du≈ºo b≈Çƒôdnych, zwiƒôksz.
CONFIDENCE_THRESHOLD = 0.5

# 4. Ustaw nazwƒô dla pliku wynikowego.
OUTPUT_FILE_PATH = "your_dataset_FAST_LABELED.csv"
# --- KONIEC KONFIGURACJI ---


# Krok 1: Wczytanie zbioru danych
try:
    df = pd.read_csv(FILE_PATH)
    print(f"\n‚úÖ Pomy≈õlnie wczytano zbi√≥r danych z {len(df)} wierszami z '{FILE_PATH}'.")
    df[RELEVANT_COLUMN] = df[RELEVANT_COLUMN].astype(int)
except FileNotFoundError:
    print(f"‚ùå B≈ÅƒÑD: Plik '{FILE_PATH}' nie zosta≈Ç znaleziony. Sprawd≈∫ ≈õcie≈ºkƒô i spr√≥buj ponownie.")
    sys.exit()

# --- WERYFIKACJA (PRZED) ---
print("\n--- STAN DANYCH (PRZED SKRYPTEM) ---")
print(f"Liczba wierszy w kolumnie '{RELEVANT_COLUMN}':")
print(df[RELEVANT_COLUMN].value_counts())
print(f"\nLiczba wierszy z pustƒÖ etykietƒÖ '{LABEL_COLUMN}': {df[LABEL_COLUMN].isnull().sum()}")
# ---

# Krok 2: Wczytanie modelu SBERT
device = 'cpu' # Jawnie ustawiamy CPU
model_name = 'all-MiniLM-L6-v2' # Szybki i dobry model
print(f"\nüîÑ Wczytywanie modelu SBERT '{model_name}' na urzƒÖdzenie '{device}'...")
model = SentenceTransformer(model_name, device=device)
print("‚úÖ Model wczytany pomy≈õlnie.")

# Krok 3: Stworzenie "Prototyp√≥w" dla ka≈ºdej etykiety (zamiast d≈Çugiego treningu)
print("\nüîÑ Tworzenie semantycznych prototyp√≥w z Twoich 3000 oznaczonych przyk≈Çad√≥w...")
labeled_df = df.dropna(subset=[LABEL_COLUMN]).copy()

# S≈Çownik do przechowywania wektor√≥w dla ka≈ºdej etykiety
label_embeddings_dict = defaultdict(list)

# Zakoduj wszystkie teksty z oznaczonych danych
# U≈ºywamy batch_size, aby efektywnie zarzƒÖdzaƒá pamiƒôciƒÖ
labeled_texts = labeled_df[TEXT_COLUMN].astype(str).tolist()
labeled_embeddings = model.encode(labeled_texts, batch_size=128, show_progress_bar=True)

# Grupuj wektory wed≈Çug etykiety
for i, row in labeled_df.iterrows():
    label = row[LABEL_COLUMN]
    embedding = labeled_embeddings[i]
    label_embeddings_dict[label].append(embedding)

# Oblicz ≈õredni wektor (prototyp) dla ka≈ºdej etykiety
prototype_embeddings = []
prototype_labels = []
for label, embeddings in label_embeddings_dict.items():
    prototype_embeddings.append(np.mean(embeddings, axis=0))
    prototype_labels.append(label)

prototype_embeddings = torch.tensor(np.array(prototype_embeddings), device=device)
print(f"‚úÖ Stworzono {len(prototype_labels)} prototyp√≥w.")

# Krok 4: Zakodowanie wszystkich nieoznaczonych zda≈Ñ (krok zoptymalizowany)
print(f"\nüîÑ Kodowanie {df[LABEL_COLUMN].isnull().sum()} nieoznaczonych zda≈Ñ. To jest g≈Ç√≥wny, najd≈Çu≈ºszy krok...")
print("üí° U≈ºywamy przetwarzania r√≥wnoleg≈Çego, aby maksymalnie przyspieszyƒá ten proces.")

unlabeled_texts = df[df[LABEL_COLUMN].isnull()][TEXT_COLUMN].astype(str).tolist()

# === KLUCZOWA OPTYMALIZACJA PRƒòDKO≈öCI ===
# Uruchamiamy pule proces√≥w, aby u≈ºyƒá wielu rdzeni CPU
pool = model.start_multi_process_pool()
# Kodujemy zdania, praca jest automatycznie rozdzielana na wiele proces√≥w
unlabeled_embeddings = model.encode_multi_process(unlabeled_texts, pool, batch_size=128)
model.stop_multi_process_pool(pool)
# ==========================================

unlabeled_embeddings = torch.tensor(unlabeled_embeddings, device=device)
print("‚úÖ Kodowanie nieoznaczonych zda≈Ñ zako≈Ñczone.")

# Krok 5: Wyszukiwanie semantyczne - znalezienie najbli≈ºszego prototypu dla ka≈ºdego zdania
print("\nüîÑ Wyszukiwanie najbli≈ºszych etykiet dla ka≈ºdego zdania...")
# util.semantic_search jest ekstremalnie szybkƒÖ operacjƒÖ
hits = util.semantic_search(unlabeled_embeddings, prototype_embeddings, top_k=1)
print("‚úÖ Wyszukiwanie zako≈Ñczone.")

# Krok 6: Zastosowanie zmian na podstawie progu pewno≈õci
print(f"\nüîÑ Stosowanie zmian dla wynik√≥w z pewno≈õciƒÖ >= {CONFIDENCE_THRESHOLD}...")
update_indices = []
new_labels = []
relevance_flipped_count = 0

unlabeled_df_indices = df[df[LABEL_COLUMN].isnull()].index

for i, hit_list in enumerate(hits):
    if not hit_list:
        continue
    
    best_hit = hit_list[0]
    confidence_score = best_hit['score']
    
    if confidence_score >= CONFIDENCE_THRESHOLD:
        original_df_index = unlabeled_df_indices[i]
        
        # Zapisz indeks i nowƒÖ etykietƒô
        update_indices.append(original_df_index)
        new_labels.append(prototype_labels[best_hit['corpus_id']])
        
        # Sprawd≈∫, czy RELEVANT zostanie zmieniony
        if df.loc[original_df_index, RELEVANT_COLUMN] == 0:
            relevance_flipped_count += 1

# Zastosuj zmiany w DataFrame w jednej, szybkiej operacji
if update_indices:
    df.loc[update_indices, LABEL_COLUMN] = new_labels
    df.loc[update_indices, RELEVANT_COLUMN] = 1
    print(f"‚úÖ Zastosowano {len(update_indices)} nowych etykiet.")
    print(f"‚úÖ Zmieniono flagƒô '{RELEVANT_COLUMN}' z 0 na 1 dla {relevance_flipped_count} wierszy.")
else:
    print("\n‚ö†Ô∏è ≈ªadne zdanie nie osiƒÖgnƒô≈Ço progu pewno≈õci. Nie wprowadzono ≈ºadnych zmian.")
    print("üí° Spr√≥buj obni≈ºyƒá CONFIDENCE_THRESHOLD na g√≥rze skryptu, je≈õli oczekujesz wiƒôcej dopasowa≈Ñ.")

# --- WERYFIKACJA (PO) ---
print("\n--- STAN DANYCH (PO SKRYPCIE) ---")
print(f"Liczba wierszy w kolumnie '{RELEVANT_COLUMN}' (powinna byƒá zaktualizowana):")
print(df[RELEVANT_COLUMN].value_counts())
print(f"\nLiczba wierszy z pustƒÖ etykietƒÖ '{LABEL_COLUMN}': {df[LABEL_COLUMN].isnull().sum()}")
# ---

# Krok 7: Zapisanie wyniku ko≈Ñcowego
df.to_csv(OUTPUT_FILE_PATH, index=False)
print(f"\n‚úÖ Wszystko gotowe! Zaktualizowany zbi√≥r danych zosta≈Ç zapisany w: '{OUTPUT_FILE_PATH}'")
